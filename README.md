# ğŸ¢ Azure Data Engineering Project â€“ Customer, Product & Sales Analytics

This project showcases a complete end-to-end data engineering solution using Azure services, focused on analyzing **customer behavior**, **product performance**, and **sales trends**. It demonstrates how to build scalable data pipelines and warehouses using Azure Data Factory, Databricks (Apache Spark), Azure Synapse Analytics, and Power BI.

---
 
## ğŸ“Š Business Use Case

A retail company wants to:
- Track **customer engagement** and regional growth
- Analyze **product categories**, pricing, and inventory trends
- Monitor **sales performance** across channels and time
- Enable near real-time reporting for decision-makers

---

## ğŸ“– Introduction

This project demonstrates how to design and implement an end-to-end data pipeline using Azure services. It extracts data from an API, processes and transforms it using Databricks and Apache Spark, and finally loads it into Azure Synapse Analytics for analysis and visualization via Power BI.

---

## ğŸ—ï¸ Data Architecture

We follow the **Medallion Architecture** for data organization:

- **Bronze Layer**: Raw data ingestion
- **Silver Layer**: Cleansed and transformed data using PySpark
- **Gold Layer**: Aggregated and modeled data for reporting


---

## ğŸ§  Key Features

- Ingest customer, product, and sales data using **Azure Data Factory**
- Transform large datasets with **Apache Spark** in **Databricks**
- Build a data warehouse in **Azure Synapse Analytics**
- Visualize KPIs in **Power BI Dashboards**:
  - Customer acquisition trends
  - Top-performing products
  - Regional sales breakdown
  - Year-over-year sales growth
 
---

## ğŸ” Pipeline Flow Overview

1. **Ingest**: Data pulled from REST APIs (or CSV dumps)
2. **Store**: Raw data stored in Azure Data Lake (Bronze)
3. **Transform**: Using PySpark in Databricks (Silver)
4. **Warehouse**: Cleaned and modeled data loaded into Synapse (Gold)
5. **Visualize**: Connected to Power BI for stakeholder insights

---

## ğŸ“š Project Modules

- **Data Factory Pipelines** â€“ Data ingestion orchestration
- **Databricks Notebooks** â€“ Cleaning and joining customer/product/sales data
- **Spark Jobs** â€“ ETL logic for building dimension and fact tables
- **Synapse SQL Scripts** â€“ Creating external tables and views
- **Power BI Reports** â€“ Dashboards using gold layer tables

---


## ğŸ“¡ Data Understanding (API)

We ingest real-time or scheduled data from an open API, such as a weather, finance, or e-commerce dataset.  
The raw JSON/CSV data is stored in the Bronze layer of Azure Data Lake Gen2.

---

## ğŸ” Data Ingestion (Bronze Layer)

- Data is pulled using **Azure Data Factory Pipelines**
- Stored securely in **Azure Data Lake Gen2**
- Implements parameterized pipelines and dynamic datasets

---

## âš™ï¸ Creating Azure Resources

Azure services used:
- Azure Data Factory
- Azure Storage (Data Lake Gen2)
- Azure Databricks
- Azure Synapse Analytics
- Power BI (Desktop/Service)

---

## ğŸ­ Azure Data Factory Overview

- ADF is used to orchestrate data movement from API â†’ Blob Storage
- Supports triggers, monitoring, and reusable linked services

---

## ğŸ’¾ Azure Data Lake Gen2

- Secure and scalable data storage for all ingestion layers
- Supports hierarchical namespace for better folder management

---

## ğŸ”„ ETL Pipelines with Azure Data Factory

- Data ingestion from REST APIs or on-prem sources
- Dynamic pipelines with ForEach, Lookup, Web, and Copy activities
- Logs pipeline run status and failure reasons

---

## ğŸ“ˆ Real-Time Scenarios with ADF

- Trigger data every 1 hour/day using event-based or scheduled triggers
- Handle schema evolution and null values
- Integrate REST API pagination handling in pipeline

---

## âš¡ Azure Databricks Resource

- Set up Databricks using ADF or Azure Portal
- Use a notebook-based approach for transformations

---

## ğŸ”¥ Databricks Cluster & Overview

- Configure a standard or autoscaling cluster
- Attach notebooks for transformations
- Schedule jobs using Databricks Jobs or via ADF

---

## ğŸ” Service Principal

- Register Azure AD App for secure access between ADF, Databricks, and Storage
- Assign appropriate roles (Storage Blob Data Contributor, etc.)

---

## ğŸ§¼ Data Transformation with Databricks

- Read data from Bronze layer
- Clean, normalize, and flatten JSON
- Handle missing values and inconsistent formats

---

## ğŸ’¡ Apache Spark (Silver Layer)

- Use PySpark to filter, deduplicate, join, and aggregate
- Store cleaned data back in Data Lake as Parquet or Delta format

---

## ğŸ PySpark 

This section includes beginner-to-intermediate PySpark examples:
- DataFrame creation
- Joins, aggregations
- UDFs
- Partitioning and caching

---

## ğŸ“Š Big Data Analytics with PySpark

- Perform large-scale joins and aggregations
- Analyze user behavior, trends, and anomalies
- Use `window functions` for time-based analytics

---

## ğŸ›ï¸ Azure Synapse Analytics (Gold Layer)

- Load curated data from Silver layer into Synapse
- Use PolyBase or COPY statement for ingestion
- Build optimized star/snowflake schema for BI reporting

---

## ğŸ” OPENROWSET() Function

- Query data directly from Azure Storage without loading into Synapse tables
- Great for ad-hoc analytics

